{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ctgan pyspark\n",
    "#!pip install psycopg2-binary psycopg2 SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import from_utc_timestamp\n",
    "\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "#from pyspark.sql import SQLContext\n",
    "#from pyspark.conf import SparkConf\n",
    "#from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python Spark 100M example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1f67ec3b500>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spark = SparkSession.builder.appName(\"Python Spark SQL basic example\").getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Python Spark 100M example\")\\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.cores.max\", \"4\") \\\n",
    "    .config(\"spark.driver.memory\",\"4g\") \\\n",
    "    .config(\"spark.executor.extraClassPath\", \"postgresql-42.7.4.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(row_id=0, timestamp=0, user_id=115, content_id=5692, content_type_id=False, task_container_id=1, user_answer=3, answered_correctly=1, prior_question_elapsed_time=None, prior_question_had_explanation=None),\n",
       " Row(row_id=1, timestamp=56943, user_id=115, content_id=5716, content_type_id=False, task_container_id=2, user_answer=2, answered_correctly=1, prior_question_elapsed_time=37000.0, prior_question_had_explanation=False),\n",
       " Row(row_id=2, timestamp=118363, user_id=115, content_id=128, content_type_id=False, task_container_id=0, user_answer=0, answered_correctly=1, prior_question_elapsed_time=55000.0, prior_question_had_explanation=False),\n",
       " Row(row_id=3, timestamp=131167, user_id=115, content_id=7860, content_type_id=False, task_container_id=3, user_answer=0, answered_correctly=1, prior_question_elapsed_time=19000.0, prior_question_had_explanation=False),\n",
       " Row(row_id=4, timestamp=137965, user_id=115, content_id=7922, content_type_id=False, task_container_id=4, user_answer=1, answered_correctly=1, prior_question_elapsed_time=11000.0, prior_question_had_explanation=False)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Download this file from kaggel it is around 1.4GB non compressed 100 Million rows\n",
    "## url: https://www.kaggle.com/datasets/rohanrao/riiid-train-data-multiple-formats?select=riiid_train.parquet \n",
    "\n",
    "path = 'C:/Users/piyus/Downloads/riiid_train/riiid_train.parquet'\n",
    "\n",
    "df_spark = spark.read.option(\"header\",\"true\").option(\"recursiveFileLookup\",\"true\").parquet(path)\n",
    "\n",
    "df_spark.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Both Works   #print(df_spark.schema)\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This will only show unique id's \n",
    "#df_spark.select('user_id').distinct().show()\n",
    "\n",
    "## This will show id and count\n",
    "#Use this only on small DF\n",
    "#df_spark.groupby('user_id').count().show(n=df_spark.count(), truncate = False)\n",
    "\n",
    "#df_spark.groupby('user_id').count().show(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-------+----------+---------------+-----------------+-----------+------------------+---------------------------+------------------------------+-------------------+\n",
      "|row_id|timestamp|user_id|content_id|content_type_id|task_container_id|user_answer|answered_correctly|prior_question_elapsed_time|prior_question_had_explanation|   timestamp_column|\n",
      "+------+---------+-------+----------+---------------+-----------------+-----------+------------------+---------------------------+------------------------------+-------------------+\n",
      "|     0|        0|    115|      5692|          false|                1|          3|                 1|                       NULL|                          NULL|1970-01-01 05:30:00|\n",
      "|     1|    56943|    115|      5716|          false|                2|          2|                 1|                    37000.0|                         false|1970-01-01 21:19:03|\n",
      "|     2|   118363|    115|       128|          false|                0|          0|                 1|                    55000.0|                         false|1970-01-02 14:22:43|\n",
      "|     3|   131167|    115|      7860|          false|                3|          0|                 1|                    19000.0|                         false|1970-01-02 17:56:07|\n",
      "|     4|   137965|    115|      7922|          false|                4|          1|                 1|                    11000.0|                         false|1970-01-02 19:49:25|\n",
      "|     5|   157063|    115|       156|          false|                5|          2|                 1|                     5000.0|                         false|1970-01-03 01:07:43|\n",
      "|     6|   176092|    115|        51|          false|                6|          0|                 1|                    17000.0|                         false|1970-01-03 06:24:52|\n",
      "|     7|   194190|    115|        50|          false|                7|          3|                 1|                    17000.0|                         false|1970-01-03 11:26:30|\n",
      "|     8|   212463|    115|      7896|          false|                8|          2|                 1|                    16000.0|                         false|1970-01-03 16:31:03|\n",
      "|     9|   230983|    115|      7863|          false|                9|          0|                 1|                    16000.0|                         false|1970-01-03 21:39:43|\n",
      "|    10|   255381|    115|       152|          false|               10|          2|                 0|                    17000.0|                         false|1970-01-04 04:26:21|\n",
      "|    11|   280033|    115|       104|          false|               11|          1|                 0|                    22000.0|                         false|1970-01-04 11:17:13|\n",
      "|    12|   302994|    115|       108|          false|               12|          1|                 0|                    23000.0|                         false|1970-01-04 17:39:54|\n",
      "|    13|   328686|    115|      7900|          false|               13|          0|                 1|                    21000.0|                         false|1970-01-05 00:48:06|\n",
      "|    14|   352686|    115|      7901|          false|               14|          1|                 1|                    24000.0|                         false|1970-01-05 07:28:06|\n",
      "|    15|   376162|    115|      7971|          false|               15|          2|                 1|                    22000.0|                         false|1970-01-05 13:59:22|\n",
      "|    16|   398020|    115|        25|          false|               16|          1|                 1|                    21000.0|                         false|1970-01-05 20:03:40|\n",
      "|    17|   418008|    115|       183|          false|               17|          0|                 0|                    20000.0|                         false|1970-01-06 01:36:48|\n",
      "|    18|   437272|    115|      7926|          false|               18|          1|                 1|                    18000.0|                         false|1970-01-06 06:57:52|\n",
      "|    19|   468511|    115|      7927|          false|               19|          3|                 1|                    17000.0|                         false|1970-01-06 15:38:31|\n",
      "+------+---------+-------+----------+---------------+-----------------+-----------+------------------+---------------------------+------------------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Changing timestamp from second in long format to timestamp\n",
    "# Adding new Column with transformation\n",
    "converted_df = df_spark.withColumn(\"timestamp_column\", F.from_unixtime(F.col(\"timestamp\")))\n",
    "\n",
    "converted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Changing timestamp from second in long format to timestamp Another way to do it\n",
    "#df1 = df_spark.withColumn('end_time', from_utc_timestamp(F.from_unixtime(F.col(\"timestamp\")), 'IST'))\n",
    "#df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+---------------+-----------------+-----------+------------------+---------------------------+------------------------------+-------------------+\n",
      "|row_id|user_id|content_id|content_type_id|task_container_id|user_answer|answered_correctly|prior_question_elapsed_time|prior_question_had_explanation|          timestamp|\n",
      "+------+-------+----------+---------------+-----------------+-----------+------------------+---------------------------+------------------------------+-------------------+\n",
      "|     0|    115|      5692|          false|                1|          3|                 1|                       NULL|                          NULL|1970-01-01 05:30:00|\n",
      "|     1|    115|      5716|          false|                2|          2|                 1|                    37000.0|                         false|1970-01-01 21:19:03|\n",
      "|     2|    115|       128|          false|                0|          0|                 1|                    55000.0|                         false|1970-01-02 14:22:43|\n",
      "|     3|    115|      7860|          false|                3|          0|                 1|                    19000.0|                         false|1970-01-02 17:56:07|\n",
      "|     4|    115|      7922|          false|                4|          1|                 1|                    11000.0|                         false|1970-01-02 19:49:25|\n",
      "|     5|    115|       156|          false|                5|          2|                 1|                     5000.0|                         false|1970-01-03 01:07:43|\n",
      "|     6|    115|        51|          false|                6|          0|                 1|                    17000.0|                         false|1970-01-03 06:24:52|\n",
      "|     7|    115|        50|          false|                7|          3|                 1|                    17000.0|                         false|1970-01-03 11:26:30|\n",
      "|     8|    115|      7896|          false|                8|          2|                 1|                    16000.0|                         false|1970-01-03 16:31:03|\n",
      "|     9|    115|      7863|          false|                9|          0|                 1|                    16000.0|                         false|1970-01-03 21:39:43|\n",
      "|    10|    115|       152|          false|               10|          2|                 0|                    17000.0|                         false|1970-01-04 04:26:21|\n",
      "|    11|    115|       104|          false|               11|          1|                 0|                    22000.0|                         false|1970-01-04 11:17:13|\n",
      "|    12|    115|       108|          false|               12|          1|                 0|                    23000.0|                         false|1970-01-04 17:39:54|\n",
      "|    13|    115|      7900|          false|               13|          0|                 1|                    21000.0|                         false|1970-01-05 00:48:06|\n",
      "|    14|    115|      7901|          false|               14|          1|                 1|                    24000.0|                         false|1970-01-05 07:28:06|\n",
      "|    15|    115|      7971|          false|               15|          2|                 1|                    22000.0|                         false|1970-01-05 13:59:22|\n",
      "|    16|    115|        25|          false|               16|          1|                 1|                    21000.0|                         false|1970-01-05 20:03:40|\n",
      "|    17|    115|       183|          false|               17|          0|                 0|                    20000.0|                         false|1970-01-06 01:36:48|\n",
      "|    18|    115|      7926|          false|               18|          1|                 1|                    18000.0|                         false|1970-01-06 06:57:52|\n",
      "|    19|    115|      7927|          false|               19|          3|                 1|                    17000.0|                         false|1970-01-06 15:38:31|\n",
      "+------+-------+----------+---------------+-----------------+-----------+------------------+---------------------------+------------------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove old column\n",
    "converted_df = converted_df.drop('timestamp')\n",
    "# rename new column\n",
    "converted_df = converted_df.withColumnRenamed('timestamp_column','timestamp') \n",
    "\n",
    "converted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to DB !!\n"
     ]
    }
   ],
   "source": [
    "## Create Connection to PostgreSQL.\n",
    "try:\n",
    "    conn = psycopg2.connect(dbname=\"postgres\", user='postgres', password='password', port='5432')\n",
    "    print(\"Connected to DB !!\")\n",
    "except:\n",
    "    print(\"Connection was unsuccessful !!\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- row_id: long (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- content_id: short (nullable = true)\n",
      " |-- content_type_id: boolean (nullable = true)\n",
      " |-- task_container_id: short (nullable = true)\n",
      " |-- user_answer: byte (nullable = true)\n",
      " |-- answered_correctly: byte (nullable = true)\n",
      " |-- prior_question_elapsed_time: float (nullable = true)\n",
      " |-- prior_question_had_explanation: boolean (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "converted_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS testdb.consent (\n",
    "        row_id BIGINT primary key,\n",
    "        timestamp TIMESTAMP,\n",
    "        user_id varchar(20),\n",
    "        content_id INT,  \n",
    "\t\tcontent_type_id boolean,\n",
    "\t\ttask_container_id INT,\n",
    "\t\tuser_answer INT,\n",
    "\t\tanswered_correctly INT,\n",
    "\t\tprior_question_elapsed_time BIGINT,\n",
    "\t\tprior_question_had_explanation boolean)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 days 00:12:23.341035\n"
     ]
    }
   ],
   "source": [
    "## You have to keep the jar in right location then only spark will identify it \n",
    "## For windows put in spark folder ==> C:\\Program Files\\spark-3.5.3-bin-hadoop3\\jars\n",
    "## Databricks have diff location\n",
    "\n",
    "url_connect = 'jdbc:postgresql://localhost:5432/postgres?currentSchema=testdb'\n",
    "properties = {\"user\": \"postgres\",\"password\": \"password\",\"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "start = pd.Timestamp.now()\n",
    "\n",
    "converted_df.write.jdbc(url=url_connect, table=\"consent\", mode=\"overwrite\", properties=properties)\n",
    "\n",
    "print(pd.Timestamp.now()-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
